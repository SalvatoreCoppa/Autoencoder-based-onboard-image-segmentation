{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1LrjYaiGrCuinYiJ6vgekbhQnDuDShfTk",
     "timestamp": 1750931752966
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "conda-env-mygpuenv-py",
   "display_name": "Python [conda env:mygpuenv]",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Percorso al file zip\n",
    "zip_path = \"./dataset.zip\"\n",
    "\n",
    "# Percorso di estrazione (stessa cartella, sottocartella 'dataset/')\n",
    "extract_path = \"./dataset/\"\n",
    "\n",
    "# Estrai solo se la cartella non esiste\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Estrazione in corso...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Estrazione completata.\")\n",
    "else:\n",
    "    print(\"La cartella 'dataset/' esiste gi√†. Estrazione saltata.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoZzIaYbdlaf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751445582153,
     "user_tz": -120,
     "elapsed": 42,
     "user": {
      "displayName": "ANTONIO GRAZIOSI",
      "userId": "03033347375849537816"
     }
    },
    "outputId": "990fc5f6-6a05-4811-ed46-862bdb116779",
    "ExecuteTime": {
     "end_time": "2025-07-02T11:58:44.643886Z",
     "start_time": "2025-07-02T11:58:44.597099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cartella 'dataset/' esiste gi√†. Estrazione saltata.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Analisi del dataset: distribuzione delle classi**\n",
    "\n",
    "Questa sezione esegue una prima analisi esplorativa sul dataset. Dopo aver aggiunto il percorso personalizzato al `sys.path` (per poter importare moduli personalizzati dal progetto), viene utilizzata la funzione `analyze_class_distribution()` da `analysis_utils`.\n",
    "Questa funzione esamina le maschere di segmentazione all‚Äôinterno del dataset per calcolare:\n",
    "\n",
    "*   La percentuale di pixel occupata da ciascuna classe\n",
    "*   La presenza di ciascuna classe in termini di immagini in cui appare\n",
    "\n",
    "Il risultato √® un `DataFrame` Pandas con una rappresentazione tabellare della distribuzione di tutte le 9 classi, visualizzato con formattazione."
   ],
   "metadata": {
    "id": "aJ_8-8ZIF7uI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#ANALISI DATASET, RESTITUISCE UN DATA\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Importa la funzione di analisi\n",
    "from utils.analysis_utils import analyze_class_distribution\n",
    "\n",
    "# Specifica il percorso del dataset\n",
    "DATASET_DIR = './dataset/dataset/'\n",
    "\n",
    "# Esegui l'analisi\n",
    "df = analyze_class_distribution(DATASET_DIR)\n",
    "\n",
    "# Mostra il DataFrame in forma tabellare\n",
    "df.style.set_caption(\"Distribuzione delle Classi\").format(precision=2)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "TN8Is6dIkLIq",
    "outputId": "0bbefc7a-fb5e-43dd-d7c0-a9958270004e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751446663305,
     "user_tz": -120,
     "elapsed": 7989,
     "user": {
      "displayName": "ANTONIO GRAZIOSI",
      "userId": "03033347375849537816"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-07-02T11:59:05.477517Z",
     "start_time": "2025-07-02T11:58:54.786460Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 932/932 [00:09<00:00, 99.15it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x220ddbc3ca0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cfc25\">\n",
       "  <caption>Distribuzione delle Classi</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cfc25_level0_col0\" class=\"col_heading level0 col0\" >Class</th>\n",
       "      <th id=\"T_cfc25_level0_col1\" class=\"col_heading level0 col1\" >Pixel %</th>\n",
       "      <th id=\"T_cfc25_level0_col2\" class=\"col_heading level0 col2\" >Presence % (images)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cfc25_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_cfc25_row0_col1\" class=\"data row0 col1\" >2.86</td>\n",
       "      <td id=\"T_cfc25_row0_col2\" class=\"data row0 col2\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cfc25_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_cfc25_row1_col1\" class=\"data row1 col1\" >15.04</td>\n",
       "      <td id=\"T_cfc25_row1_col2\" class=\"data row1 col2\" >58.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cfc25_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_cfc25_row2_col1\" class=\"data row2 col1\" >13.36</td>\n",
       "      <td id=\"T_cfc25_row2_col2\" class=\"data row2 col2\" >68.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cfc25_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_cfc25_row3_col1\" class=\"data row3 col1\" >15.20</td>\n",
       "      <td id=\"T_cfc25_row3_col2\" class=\"data row3 col2\" >55.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_cfc25_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_cfc25_row4_col1\" class=\"data row4 col1\" >0.18</td>\n",
       "      <td id=\"T_cfc25_row4_col2\" class=\"data row4 col2\" >4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_cfc25_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_cfc25_row5_col1\" class=\"data row5 col1\" >0.87</td>\n",
       "      <td id=\"T_cfc25_row5_col2\" class=\"data row5 col2\" >17.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_cfc25_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_cfc25_row6_col1\" class=\"data row6 col1\" >6.24</td>\n",
       "      <td id=\"T_cfc25_row6_col2\" class=\"data row6 col2\" >38.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_cfc25_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_cfc25_row7_col1\" class=\"data row7 col1\" >36.06</td>\n",
       "      <td id=\"T_cfc25_row7_col2\" class=\"data row7 col2\" >99.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfc25_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_cfc25_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_cfc25_row8_col1\" class=\"data row8 col1\" >10.19</td>\n",
       "      <td id=\"T_cfc25_row8_col2\" class=\"data row8 col2\" >90.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Suddivisione del dataset in Train/Val e Test con bilanciamento delle classi**\n",
    "\n",
    "### üì¶ Suddivisione del Dataset in Train/Val e Test\n",
    "\n",
    "Questa sezione gestisce la **suddivisione del dataset in due parti principali**:\n",
    "- **Train/Validation (90%)**\n",
    "- **Test set (10%)**\n",
    "\n",
    "L'obiettivo √® ottenere un **test set rappresentativo**, con una distribuzione delle classi simile a quella dell'intero dataset. Lo split viene fatto in modo **greedy** per bilanciare:\n",
    "- La distribuzione dei **pixel per classe**\n",
    "- La presenza/assenza delle classi nelle immagini\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Importazione dei moduli\n",
    "\n",
    "Vengono caricati:\n",
    "- Moduli di sistema (`os`, `sys`, `numpy`, `pandas`, ecc.)\n",
    "- Librerie custom del progetto (`class_mapping`, `split_utils`, `analysis_utils`)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìÅ Definizione dei percorsi\n",
    "\n",
    "- `PROJECT_DIR`: percorso principale del progetto\n",
    "- `DATASET_DIR`: contiene le cartelle delle immagini\n",
    "- `train_val_split.txt` e `test_split.txt`: file `.txt` con l‚Äôelenco dei campioni suddivisi\n",
    "\n",
    "---\n",
    "\n",
    "#### üîê Controllo esistenza file\n",
    "\n",
    "Per evitare di ripetere la suddivisione, il codice controlla se i file di split esistono gi√†. Se s√¨, il processo termina subito.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä Analisi delle immagini\n",
    "\n",
    "Attraverso la funzione `analyze_images_distribution()` si ottengono per ciascuna immagine:\n",
    "- La **distribuzione percentuale di pixel per classe**\n",
    "- La **presenza binaria** (classe presente/s√¨-no)\n",
    "\n",
    "Inoltre, viene calcolata la distribuzione **globale** dell'intero subset.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÄ Algoritmo di Split Greedy\n",
    "\n",
    "La funzione `greedy_split()` esegue lo split mantenendo:\n",
    "- Proporzioni simili per classe (pixel)\n",
    "- Presenza equilibrata delle classi nel test set\n",
    "\n",
    "Il parametro `test_ratio = 0.10` definisce il 10% per il test.\n",
    "\n",
    "---\n",
    "\n",
    "#### üíæ Salvataggio dei risultati\n",
    "\n",
    "I nomi delle immagini vengono scritti in:\n",
    "- `test_split.txt`: campioni del test\n",
    "- `train_val_split.txt`: campioni usati per training + validation\n",
    "\n",
    "---\n",
    "\n",
    "#### üìà Confronto tra distribuzioni\n",
    "\n",
    "Infine, viene generata una tabella `pandas` con il confronto tra:\n",
    "- **Distribuzione di pixel globale** vs test\n",
    "- **Presenza delle classi globale** vs test\n",
    "\n",
    "Questa tabella verifica la **coerenza statistica dello split**.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils.split_utils import (\n",
    "    compute_distribution,\n",
    "    compute_presence_distribution,\n",
    "    greedy_split\n",
    ")\n",
    "from utils.analysis_utils import analyze_images_distribution\n",
    "\n",
    "\n",
    "def aggregate_counts(list_of_dicts):\n",
    "    \"\"\"Somma una lista di dizionari {classe: valore} in un unico dizionario aggregato.\"\"\"\n",
    "    total = defaultdict(int)\n",
    "    for d in list_of_dicts:\n",
    "        for k, v in d.items():\n",
    "            total[k] += v\n",
    "    return total\n",
    "\n",
    "def convert_set_to_dict(class_set):\n",
    "    \"\"\"Converte un set come {1, 2} in {1: 1, 2: 1}\"\"\"\n",
    "    return {cls: 1 for cls in class_set}\n",
    "\n",
    "\n",
    "# === Percorsi ===\n",
    "PROJECT_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14\"\n",
    "DATASET_DIR = os.path.join(PROJECT_DIR, \"dataset/dataset\")\n",
    "TRAINVAL_FILE = os.path.join(PROJECT_DIR, \"train_val_split.txt\")\n",
    "TEST_FILE = os.path.join(PROJECT_DIR, \"test_split.txt\")\n",
    "\n",
    "# === Controllo esistenza split ===\n",
    "split_exists = os.path.exists(TRAINVAL_FILE) and os.path.exists(TEST_FILE)\n",
    "\n",
    "# === Caricamento split se esistono ===\n",
    "if split_exists:\n",
    "    print(\"I file di split esistono gi√†. Lo split non verr√† rieseguito.\")\n",
    "    with open(TRAINVAL_FILE, \"r\") as f:\n",
    "        trainval_set = [line.strip() for line in f.readlines()]\n",
    "    with open(TEST_FILE, \"r\") as f:\n",
    "        test_set = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    # === Carica elenco immagini completo da usare nello split ===\n",
    "    with open(TRAINVAL_FILE, \"r\") as f:\n",
    "        full_image_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# === Analisi immagini (su tutto il dataset) ===\n",
    "(\n",
    "    image_pixel_distributions,\n",
    "    image_class_presence,\n",
    "    global_pixel_dist,\n",
    "    global_presence_dist,\n",
    "    _  # immagini analizzate (non necessario qui)\n",
    ") = analyze_images_distribution(DATASET_DIR)\n",
    "\n",
    "# === Ricava CLASS_NAMES dinamicamente ===\n",
    "CLASS_NAMES = sorted(set(global_pixel_dist.keys()) | set(global_presence_dist.keys()))\n",
    "\n",
    "# === Se lo split non esiste, esegui split greedy ===\n",
    "if not split_exists:\n",
    "    print(\"Split non trovato: eseguo il greedy split.\")\n",
    "    test_ratio = 0.10\n",
    "    test_set, trainval_set, test_pixel_counts, test_class_presence = greedy_split(\n",
    "        image_pixel_distributions,\n",
    "        image_class_presence,\n",
    "        global_pixel_dist,\n",
    "        global_presence_dist,\n",
    "        CLASS_NAMES,\n",
    "        test_ratio\n",
    "    )\n",
    "\n",
    "    # === Salvataggio .txt ===\n",
    "    with open(TEST_FILE, \"w\") as f:\n",
    "        for name in test_set:\n",
    "            f.write(name + \"\\n\")\n",
    "    with open(TRAINVAL_FILE, \"w\") as f:\n",
    "        for name in trainval_set:\n",
    "            f.write(name + \"\\n\")\n",
    "else:\n",
    "    # === Recupera conteggi da immagini ===\n",
    "    test_pixel_counts = [image_pixel_distributions[n] for n in test_set]\n",
    "    test_class_presence = [image_class_presence[n] for n in test_set]\n",
    "\n",
    "# === Recupera anche trainval counts ===\n",
    "trainval_pixel_counts = [image_pixel_distributions[n] for n in trainval_set]\n",
    "trainval_class_presence = [image_class_presence[n] for n in trainval_set]\n",
    "\n",
    "# === Converti set in dizionari per aggregazione presenza ===\n",
    "test_presence_dicts = [convert_set_to_dict(s) for s in test_class_presence]\n",
    "trainval_presence_dicts = [convert_set_to_dict(s) for s in trainval_class_presence]\n",
    "\n",
    "# === Aggrega ===\n",
    "agg_test_pixel = aggregate_counts(test_pixel_counts)\n",
    "agg_trainval_pixel = aggregate_counts(trainval_pixel_counts)\n",
    "agg_test_presence = aggregate_counts(test_presence_dicts)\n",
    "agg_trainval_presence = aggregate_counts(trainval_presence_dicts)\n",
    "\n",
    "# === Calcola distribuzioni ===\n",
    "test_pixel_dist = compute_distribution(agg_test_pixel, CLASS_NAMES)\n",
    "trainval_pixel_dist = compute_distribution(agg_trainval_pixel, CLASS_NAMES)\n",
    "test_presence_dist = compute_presence_distribution(agg_test_presence, len(test_set), CLASS_NAMES)\n",
    "trainval_presence_dist = compute_presence_distribution(agg_trainval_presence, len(trainval_set), CLASS_NAMES)\n",
    "\n",
    "# === Confronto in DataFrame ===\n",
    "df = pd.DataFrame({\n",
    "    \"Classe\": CLASS_NAMES,\n",
    "    \"Pixel TrainVal (%)\": [round(trainval_pixel_dist.get(c, 0)*100, 2) for c in CLASS_NAMES],\n",
    "    \"Pixel Test (%)\":     [round(test_pixel_dist.get(c, 0)*100, 2) for c in CLASS_NAMES],\n",
    "    \"Presenza TrainVal (%)\": [round(trainval_presence_dist.get(c, 0)*100, 2) for c in CLASS_NAMES],\n",
    "    \"Presenza Test (%)\":     [round(test_presence_dist.get(c, 0)*100, 2) for c in CLASS_NAMES],\n",
    "})\n",
    "\n",
    "print(\"\\nConfronto distribuzioni TrainVal vs Test:\")\n",
    "print(df)\n"
   ],
   "metadata": {
    "id": "97pjIS7PBD6C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751447409117,
     "user_tz": -120,
     "elapsed": 16979,
     "user": {
      "displayName": "ANTONIO GRAZIOSI",
      "userId": "03033347375849537816"
     }
    },
    "outputId": "dd020502-0b1a-4b37-b030-9986aaa8bee0",
    "ExecuteTime": {
     "end_time": "2025-07-02T11:59:18.549269Z",
     "start_time": "2025-07-02T11:59:10.457182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I file di split esistono gi√†. Lo split non verr√† rieseguito.\n",
      "Analisi immagini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 932/932 [00:08<00:00, 115.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confronto distribuzioni TrainVal vs Test:\n",
      "   Classe  Pixel TrainVal (%)  Pixel Test (%)  Presenza TrainVal (%)  \\\n",
      "0       0                2.85            2.94                 100.00   \n",
      "1       1               15.04           15.06                  58.95   \n",
      "2       2               13.35           13.47                  68.97   \n",
      "3       3               15.19           15.29                  55.85   \n",
      "4       4                0.19            0.10                   4.53   \n",
      "5       5                0.86            0.87                  17.66   \n",
      "6       6                6.24            6.24                  38.07   \n",
      "7       7               36.06           36.10                  99.16   \n",
      "8       8               10.22            9.93                  90.69   \n",
      "\n",
      "   Presenza Test (%)  \n",
      "0             100.00  \n",
      "1              59.14  \n",
      "2              68.82  \n",
      "3              55.91  \n",
      "4               4.30  \n",
      "5              17.20  \n",
      "6              38.71  \n",
      "7              98.92  \n",
      "8              90.32  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Implementazione della K-Fold Cross-Validation sul Training Set**\n",
    "### üìö Suddivisione K-Fold del Dataset (Train/Val)\n",
    "\n",
    "Questa sezione realizza la **suddivisione del dataset train/val (90% dei dati)** in **K fold** per eseguire una **cross-validation stratificata**.  \n",
    "Ogni fold verr√† salvato in un file `.txt` e potr√† essere usato per addestrare e validare il modello su diverse porzioni del dataset, migliorando la generalizzazione.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Importazioni e configurazioni\n",
    "\n",
    "Vengono importati:\n",
    "- Moduli standard (`os`, `numpy`, `pandas`, ecc.)\n",
    "- Moduli custom del progetto (`class_mapping`, `split_utils`, `analysis_utils`)\n",
    "- La classe `KFold` da `sklearn.model_selection` per generare gli indici\n",
    "\n",
    "Sono inoltre definiti i percorsi:\n",
    "- `PROJECT_DIR`, `DATASET_DIR`: path base e dataset\n",
    "- `SPLIT_DIR`, `TRAIN_DIR`, `VAL_DIR`: directory in cui salvare gli split\n",
    "- `TRAINVAL_FILE`: file `.txt` con le immagini da dividere (generate precedentemente)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìÑ Caricamento e analisi delle immagini\n",
    "\n",
    "Dalla lista `train_val_split.txt`, si caricano i nomi delle immagini e si esegue un‚Äô**analisi statistica**:\n",
    "- `image_pixel_distributions`: percentuale di pixel per classe, per immagine\n",
    "- `image_class_presence`: classi presenti per immagine (s√¨/no)\n",
    "- `global_pixel_dist` e `global_presence_dist`: valori aggregati sull‚Äôintero set\n",
    "- `images_list`: lista effettiva delle immagini analizzate\n",
    "\n",
    "Queste metriche potranno essere usate per valutare la qualit√† dello split (anche se non direttamente in questa cella).\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÄ K-Fold Cross-Validation (K=5)\n",
    "\n",
    "Utilizzando `KFold` con `shuffle=True` e `random_state=42`, vengono generati 5 fold con indici diversi per ogni split. Per ogni fold:\n",
    "- Si costruisce il `train_set` e il `val_set`\n",
    "- Si salvano due file:\n",
    "  - `train_foldN.txt` ‚Üí immagini per l'addestramento\n",
    "  - `val_foldN.txt` ‚Üí immagini per la validazione\n",
    "\n",
    "Questi file verranno usati nel training loop per caricare dinamicamente i dati in base al fold.\n",
    "\n",
    "---\n",
    "\n",
    "#### üíæ Output\n"
   ],
   "metadata": {
    "id": "cF-wFvPPHR-N"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LLpsT_3A6nw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751447646753,
     "user_tz": -120,
     "elapsed": 28149,
     "user": {
      "displayName": "ANTONIO GRAZIOSI",
      "userId": "03033347375849537816"
     }
    },
    "outputId": "9efe56dc-f090-425c-fdd9-0496a8621d63",
    "ExecuteTime": {
     "end_time": "2025-07-02T12:00:30.804652Z",
     "start_time": "2025-07-02T12:00:21.022587Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils.split_utils import compute_distribution, compute_presence_distribution, compute_distance\n",
    "from utils.analysis_utils import analyze_images_distribution\n",
    "\n",
    "# === CONFIG ===\n",
    "K = 5\n",
    "PROJECT_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14\"\n",
    "DATASET_DIR = os.path.join(PROJECT_DIR, \"dataset/dataset\")\n",
    "SPLIT_DIR = os.path.join(PROJECT_DIR, \"kfold_splits\")\n",
    "TRAINVAL_FILE = os.path.join(PROJECT_DIR, \"train_val_split.txt\")\n",
    "TRAIN_DIR = os.path.join(SPLIT_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(SPLIT_DIR, \"val\")\n",
    "\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "# === Carica elenco immagini da usare nello split ===\n",
    "with open(TRAINVAL_FILE, \"r\") as f:\n",
    "    images_subset = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# === Analisi immagini solo su train_val_split.txt ===\n",
    "(\n",
    "    image_pixel_distributions,\n",
    "    image_class_presence,\n",
    "    global_pixel_dist,\n",
    "    global_presence_dist,\n",
    "    images_list\n",
    ") = analyze_images_distribution(DATASET_DIR, subset=images_subset)\n",
    "total_images = len(images_list)\n",
    "\n",
    "# === K-FOLD SPLIT ===\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "\n",
    "for train_indices, val_indices in kf.split(images_list):\n",
    "    train_path = os.path.join(TRAIN_DIR, f\"train_fold{fold}.txt\")\n",
    "    val_path = os.path.join(VAL_DIR, f\"val_fold{fold}.txt\")\n",
    "\n",
    "    # Salta fold se entrambi i file esistono gi√†\n",
    "    if os.path.exists(train_path) and os.path.exists(val_path):\n",
    "        with open(train_path, \"r\") as f:\n",
    "            train_count = len(f.readlines())\n",
    "        with open(val_path, \"r\") as f:\n",
    "            val_count = len(f.readlines())\n",
    "\n",
    "        print(f\"Fold {fold} gi√† esistente: {train_count} train, {val_count} val\")\n",
    "        fold += 1\n",
    "        continue\n",
    "\n",
    "    train_set = [images_list[i] for i in train_indices]\n",
    "    val_set = [images_list[i] for i in val_indices]\n",
    "\n",
    "    # Salvataggio\n",
    "    with open(train_path, \"w\") as f:\n",
    "        for name in train_set:\n",
    "            f.write(name + \"\\n\")\n",
    "\n",
    "    with open(val_path, \"w\") as f:\n",
    "        for name in val_set:\n",
    "            f.write(name + \"\\n\")\n",
    "\n",
    "    print(f\"Fold {fold} salvato: {len(train_set)} train, {len(val_set)} val\")\n",
    "    fold += 1\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi immagini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 838/838 [00:07<00:00, 115.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 gi√† esistente: 671 train, 167 val\n",
      "Fold 2 gi√† esistente: 671 train, 167 val\n",
      "Fold 3 gi√† esistente: 670 train, 168 val\n",
      "Fold 4 gi√† esistente: 670 train, 168 val\n",
      "Fold 5 gi√† esistente: 670 train, 168 val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Analisi delle Distribuzioni di Train e Validation nei K Fold**\n",
    "\n",
    "Questa sezione verifica che la suddivisione in **train/val per ogni fold** della K-Fold cross-validation (K=10) mantenga una **distribuzione equilibrata delle classi** rispetto al dataset globale.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Import e configurazione\n",
    "\n",
    "- Vengono importati moduli standard (`numpy`, `pandas`, `PIL`, ecc.) e moduli custom come `class_mapping`, `split_utils` e `analysis_utils`.\n",
    "- Sono definiti i percorsi al dataset e alle directory dove sono salvati i file `.txt` generati nella fase di split precedente.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìà Analisi Globale\n",
    "\n",
    "La funzione `analyze_images_distribution()` calcola:\n",
    "- La **distribuzione dei pixel per classe** sull‚Äôintero dataset\n",
    "- La **presenza percentuale** di ciascuna classe nelle immagini\n",
    "Questa analisi serve come riferimento per confrontare i singoli fold.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ Funzione: `analyze_split_distribution()`\n",
    "\n",
    "Questa funzione esegue la stessa analisi della distribuzione:\n",
    "- Su una lista di immagini (`split_list`)\n",
    "- Analizza il file `labels.png` per ciascuna cartella immagine\n",
    "- Conta il numero di pixel per ogni classe (`pixel_counts`)\n",
    "- Conta quante immagini contengono ogni classe (`presence_counts`)\n",
    "\n",
    "Il tutto viene convertito in distribuzioni normalizzate:\n",
    "- `pixel_dist` (% pixel per classe)\n",
    "- `presence_dist` (% immagini che contengono ciascuna classe)\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç Ciclo su tutti i fold\n",
    "\n",
    "Per ciascun fold da 1 a K:\n",
    "- Si leggono i file `train_foldN.txt` e `val_foldN.txt`\n",
    "- Si analizzano train e validation separatamente con `analyze_split_distribution()`\n",
    "- I risultati vengono confrontati con i valori globali\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä Output finale\n",
    "\n",
    "Per ogni fold viene stampata una tabella `pandas` che permette di verificare se la **distribuzione di classi nei fold** √® rappresentativa e bilanciata rispetto all‚Äôintero dataset.\n",
    "\n",
    "---\n",
    "\n",
    "üìå Questo passaggio √® fondamentale per **validare la correttezza dello split**: se uno o pi√π fold sono sbilanciati, le performance del modello potrebbero essere falsamente elevate o degradate.\n"
   ],
   "metadata": {
    "id": "K4pKOrR-H3ts"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.split_utils import compute_distribution, compute_presence_distribution\n",
    "from utils.analysis_utils import analyze_images_distribution\n",
    "\n",
    "# === CONFIG ===\n",
    "PROJECT_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14\"\n",
    "DATASET_DIR = os.path.join(PROJECT_DIR, \"dataset/dataset\")\n",
    "SPLIT_DIR = os.path.join(PROJECT_DIR, \"kfold_splits\")\n",
    "TRAIN_DIR = os.path.join(SPLIT_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(SPLIT_DIR, \"val\")\n",
    "K = 5\n",
    "NUM_CLASSES = 9\n",
    "CLASS_NAMES = list(range(NUM_CLASSES))  # coerente con gli ID delle classi\n",
    "\n",
    "# === Analisi globale con funzione centralizzata ===\n",
    "print(\"Analisi distribuzione globale...\")\n",
    "_, _, global_pixel_dist, global_presence_dist, images_list = analyze_images_distribution(DATASET_DIR)\n",
    "total_global = len(images_list)\n",
    "\n",
    "# === Analisi di ogni fold ===\n",
    "def analyze_split_distribution(split_list):\n",
    "    pixel_counts = defaultdict(int)\n",
    "    presence_counts = defaultdict(int)\n",
    "\n",
    "    for folder in tqdm(split_list, desc=\"Analisi split\"):\n",
    "        label_path = os.path.join(DATASET_DIR, folder, \"labels.png\")\n",
    "        if not os.path.isfile(label_path):\n",
    "            continue\n",
    "\n",
    "        label = np.array(Image.open(label_path), dtype=np.uint8).flatten()\n",
    "        unique_classes = np.unique(label)\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            count = np.sum(label == cls)\n",
    "            if count > 0:\n",
    "                pixel_counts[cls] += count\n",
    "                presence_counts[cls] += 1\n",
    "\n",
    "    total_images = len(split_list)\n",
    "    pixel_dist = compute_distribution(pixel_counts, CLASS_NAMES)\n",
    "    presence_dist = compute_presence_distribution(presence_counts, total_images, CLASS_NAMES)\n",
    "    return pixel_dist, presence_dist\n",
    "\n",
    "# === Analizza e confronta ogni fold ===\n",
    "for fold in range(1, K + 1):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    train_file = os.path.join(TRAIN_DIR, f\"train_fold{fold}.txt\")\n",
    "    val_file = os.path.join(VAL_DIR, f\"val_fold{fold}.txt\")\n",
    "\n",
    "    with open(train_file, \"r\") as f:\n",
    "        train_list = [line.strip() for line in f.readlines()]\n",
    "    with open(val_file, \"r\") as f:\n",
    "        val_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    train_pixel_dist, train_presence_dist = analyze_split_distribution(train_list)\n",
    "    val_pixel_dist, val_presence_dist = analyze_split_distribution(val_list)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Classe\": CLASS_NAMES,\n",
    "        \"Globale (Pixel%)\": [round(global_pixel_dist[c] * 100, 2) for c in CLASS_NAMES],\n",
    "        \"Train (Pixel%)\": [round(train_pixel_dist[c] * 100, 2) for c in CLASS_NAMES],\n",
    "        \"Val (Pixel%)\": [round(val_pixel_dist[c] * 100, 2) for c in CLASS_NAMES],\n",
    "        \"Globale (Presenza%)\": [round(global_presence_dist[c] * 100, 2) for c in CLASS_NAMES],\n",
    "        \"Train (Presenza%)\": [round(train_presence_dist[c] * 100, 2) for c in CLASS_NAMES],\n",
    "        \"Val (Presenza%)\": [round(val_presence_dist[c] * 100, 2) for c in CLASS_NAMES],\n",
    "    })\n",
    "\n",
    "    print(df.to_string(index=False))\n"
   ],
   "metadata": {
    "id": "hL0Ilv9yl4jx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751448569626,
     "user_tz": -120,
     "elapsed": 95896,
     "user": {
      "displayName": "ANTONIO GRAZIOSI",
      "userId": "03033347375849537816"
     }
    },
    "outputId": "277aeffc-263c-486c-98a4-97e591e888b5",
    "ExecuteTime": {
     "end_time": "2025-07-02T12:01:56.577297Z",
     "start_time": "2025-07-02T12:01:07.860266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi distribuzione globale...\n",
      "Analisi immagini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 932/932 [00:08<00:00, 104.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 671/671 [00:06<00:00, 111.76it/s]\n",
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:01<00:00, 109.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classe  Globale (Pixel%)  Train (Pixel%)  Val (Pixel%)  Globale (Presenza%)  Train (Presenza%)  Val (Presenza%)\n",
      "      0              2.86            2.85          2.87               100.00             100.00           100.00\n",
      "      1             15.04           15.07         14.92                58.97              59.02            58.68\n",
      "      2             13.36           13.37         13.27                68.96              69.00            68.86\n",
      "      3             15.20           15.21         15.09                55.85              55.89            55.69\n",
      "      4              0.18            0.19          0.22                 4.51               4.47             4.79\n",
      "      5              0.87            0.86          0.90                17.62              17.73            17.37\n",
      "      6              6.24            6.21          6.37                38.13              38.00            38.32\n",
      "      7             36.06           36.03         36.16                99.14              99.11            99.40\n",
      "      8             10.19           10.22         10.21                90.66              90.61            91.02\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 671/671 [00:06<00:00, 107.84it/s]\n",
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:01<00:00, 106.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classe  Globale (Pixel%)  Train (Pixel%)  Val (Pixel%)  Globale (Presenza%)  Train (Presenza%)  Val (Presenza%)\n",
      "      0              2.86            2.84          2.92               100.00             100.00           100.00\n",
      "      1             15.04           15.04         15.03                58.97              58.87            59.28\n",
      "      2             13.36           13.36         13.30                68.96              69.00            68.86\n",
      "      3             15.20           15.21         15.08                55.85              55.89            55.69\n",
      "      4              0.18            0.19          0.19                 4.51               4.62             4.19\n",
      "      5              0.87            0.87          0.83                17.62              17.59            17.96\n",
      "      6              6.24            6.25          6.22                38.13              38.00            38.32\n",
      "      7             36.06           36.04         36.14                99.14              99.11            99.40\n",
      "      8             10.19           10.20         10.29                90.66              90.61            91.02\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670/670 [00:06<00:00, 110.94it/s]\n",
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:01<00:00, 107.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classe  Globale (Pixel%)  Train (Pixel%)  Val (Pixel%)  Globale (Presenza%)  Train (Presenza%)  Val (Presenza%)\n",
      "      0              2.86            2.86          2.82               100.00             100.00           100.00\n",
      "      1             15.04           15.06         14.97                58.97              58.96            58.93\n",
      "      2             13.36           13.33         13.43                68.96              68.96            69.05\n",
      "      3             15.20           15.19         15.19                55.85              55.82            55.95\n",
      "      4              0.18            0.19          0.19                 4.51               4.48             4.76\n",
      "      5              0.87            0.86          0.87                17.62              17.61            17.86\n",
      "      6              6.24            6.22          6.32                38.13              38.06            38.10\n",
      "      7             36.06           36.06         36.04                99.14              99.10            99.40\n",
      "      8             10.19           10.23         10.16                90.66              90.75            90.48\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670/670 [00:05<00:00, 112.99it/s]\n",
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:01<00:00, 114.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classe  Globale (Pixel%)  Train (Pixel%)  Val (Pixel%)  Globale (Presenza%)  Train (Presenza%)  Val (Presenza%)\n",
      "      0              2.86            2.85          2.84               100.00             100.00           100.00\n",
      "      1             15.04           15.04         15.04                58.97              58.96            58.93\n",
      "      2             13.36           13.35         13.34                68.96              68.96            69.05\n",
      "      3             15.20           15.14         15.37                55.85              55.82            55.95\n",
      "      4              0.18            0.19          0.19                 4.51               4.48             4.76\n",
      "      5              0.87            0.86          0.88                17.62              17.61            17.86\n",
      "      6              6.24            6.26          6.17                38.13              38.06            38.10\n",
      "      7             36.06           36.06         36.05                99.14              99.10            99.40\n",
      "      8             10.19           10.24         10.12                90.66              90.75            90.48\n",
      "\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670/670 [00:07<00:00, 88.51it/s] \n",
      "Analisi split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:01<00:00, 91.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classe  Globale (Pixel%)  Train (Pixel%)  Val (Pixel%)  Globale (Presenza%)  Train (Presenza%)  Val (Presenza%)\n",
      "      0              2.86            2.86          2.81               100.00             100.00           100.00\n",
      "      1             15.04           14.99         15.23                58.97              58.96            58.93\n",
      "      2             13.36           13.33         13.40                68.96              68.96            69.05\n",
      "      3             15.20           15.18         15.20                55.85              55.82            55.95\n",
      "      4              0.18            0.20          0.18                 4.51               4.63             4.17\n",
      "      5              0.87            0.87          0.85                17.62              17.76            17.26\n",
      "      6              6.24            6.27          6.13                38.13              38.21            37.50\n",
      "      7             36.06           36.10         35.89                99.14              99.40            98.21\n",
      "      8             10.19           10.19         10.31                90.66              90.75            90.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Addestramento del modello DeepLabV3 + MobileNetV3 con K-Fold**\n",
    "\n",
    "Questa sezione esegue il training del modello DeepLabV3 con backbone MobileNetV3 utilizzando la **K-Fold Cross Validation**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Setup e Modello\n",
    "\n",
    "- Inizializzazione del modello `DeepLabV3` con pesi preaddestrati\n",
    "- Definizione della **loss combinata** (`CombinedLoss`), composta da Jaccard loss e Cross Entropy\n",
    "- Configurazione dell‚Äôottimizzatore e learning rate scheduler\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ Ciclo di Addestramento per ciascun Fold\n",
    "\n",
    "Per ogni fold:\n",
    "\n",
    "- Creazione dei `DataLoader` per i subset `train` e `val`\n",
    "- Training per un numero massimo di epoche (con early stopping)\n",
    "- Calcolo delle metriche: **loss, accuracy e mIoU**\n",
    "- Salvataggio del **miglior modello** in base alla mIoU\n",
    "- Monitoraggio della **memoria GPU** utilizzata ad ogni epoca\n",
    "\n",
    "---\n",
    "\n",
    "#### üíæ Output\n",
    "\n",
    "Al termine di ogni fold viene salvato:\n",
    "\n",
    "- Il modello (`.pt`) migliore\n",
    "- I valori delle metriche per ogni epoca\n",
    "- Eventuali log di debug e monitoraggio GPU\n",
    "\n",
    "---\n",
    "\n",
    "üìå Questa pipeline permette di **valutare la robustezza del modello** su diversi subset del dataset, fornendo una stima pi√π realistica delle sue performance."
   ],
   "metadata": {
    "id": "44BgQOiiJMSM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Imposta seed per riproducibilit√†\n",
    "SEED = 14\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import SegmentationDataset\n",
    "from utils.model import DeepLabV3PlusWithMobileNet\n",
    "from utils.losses import get_weighted_crossentropy_loss, combined_loss\n",
    "from utils.metrics import compute_miou, compute_pixel_accuracy\n",
    "\n",
    "# CONFIG\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 9\n",
    "EPOCHS = 50\n",
    "ALPHA = 0.7\n",
    "LR = 1e-4\n",
    "MIN_DELTA = 1e-4\n",
    "PATIENCE = 10\n",
    "MAX_MEMORY_MB = 5120  # 5 GB\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold in [5]:\n",
    "    print(f\"\\n Inizio Fold {fold}\\n{'-'*40}\")\n",
    "\n",
    "    train_dataset = SegmentationDataset(\n",
    "        root_dir=\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset\",\n",
    "        id_list_file=f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_fold{fold}.txt\",\n",
    "        image_size=IMAGE_SIZE,\n",
    "    )\n",
    "    val_dataset = SegmentationDataset(\n",
    "        root_dir=\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset\",\n",
    "        id_list_file=f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/val/val_fold{fold}.txt\",\n",
    "        image_size=IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = DeepLabV3PlusWithMobileNet(num_classes=NUM_CLASSES).to(device)\n",
    "    ce_loss_fn = get_weighted_crossentropy_loss(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    best_miou = -1.0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    fold_results = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_miou = 0.0\n",
    "        total_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "                val_loss += loss.item()\n",
    "                total_miou += compute_miou(outputs, labels, num_classes=NUM_CLASSES, ignore_index=0)\n",
    "                total_acc += compute_pixel_accuracy(outputs, labels, ignore_index=0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        avg_miou = total_miou / len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        avg_acc = total_acc / len(val_loader)\n",
    "\n",
    "        fold_results.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"mIoU\": avg_miou,\n",
    "            \"accuracy\": avg_acc\n",
    "        })\n",
    "\n",
    "        print(f\"[Fold {fold} | Epoch {epoch:02d}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "              f\"mIoU: {avg_miou:.4f} | Acc: {avg_acc:.4f}\")\n",
    "\n",
    "        if avg_miou > best_miou:\n",
    "            best_miou = avg_miou\n",
    "            epochs_no_improve = 0\n",
    "            model_path = f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/best_model_fold{fold}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Salvataggio modello Fold {fold} | Nuova mIoU: {best_miou:.4f}\")\n",
    "\n",
    "        if epoch == 1 or val_loss < best_val_loss - MIN_DELTA:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"EARLY STOPPING FOLD {fold}: no val_loss improvement for {PATIENCE} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            mem_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            if mem_allocated > MAX_MEMORY_MB:\n",
    "                print(f\"GPU MEMORY LIMIT EXCEEDED: {mem_allocated:.2f} MB\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    RESULTS.append({\n",
    "        \"fold\": fold,\n",
    "        \"best_miou\": best_miou,\n",
    "        \"epoch_results\": fold_results\n",
    "    })\n",
    "\n",
    "# ‚úÖ Salva i risultati totali in un file\n",
    "import json\n",
    "with open(\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_results.json\", \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "\n",
    "print(\"\\nCross-validation completata. Tutti i modelli e risultati salvati.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0XAhCBUebwK",
    "outputId": "0b6ca7f0-77ca-4aa0-b5fe-e65aa8dff31a",
    "ExecuteTime": {
     "end_time": "2025-07-02T14:28:13.285313Z",
     "start_time": "2025-07-02T12:48:35.821478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inizio Fold 5\n",
      "----------------------------------------\n",
      "[Fold 5 | Epoch 01] Train Loss: 1.1939 | Val Loss: 0.9365 | mIoU: 0.3788 | Acc: 0.7315\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.3788\n",
      "[Fold 5 | Epoch 02] Train Loss: 0.9250 | Val Loss: 0.7682 | mIoU: 0.4775 | Acc: 0.7943\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.4775\n",
      "[Fold 5 | Epoch 03] Train Loss: 0.7992 | Val Loss: 0.7168 | mIoU: 0.5013 | Acc: 0.7974\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5013\n",
      "[Fold 5 | Epoch 04] Train Loss: 0.7068 | Val Loss: 0.6117 | mIoU: 0.5156 | Acc: 0.8125\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5156\n",
      "[Fold 5 | Epoch 05] Train Loss: 0.6465 | Val Loss: 0.6071 | mIoU: 0.5134 | Acc: 0.8103\n",
      "[Fold 5 | Epoch 06] Train Loss: 0.5898 | Val Loss: 0.5891 | mIoU: 0.5178 | Acc: 0.8114\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5178\n",
      "[Fold 5 | Epoch 07] Train Loss: 0.5508 | Val Loss: 0.5941 | mIoU: 0.4896 | Acc: 0.7866\n",
      "[Fold 5 | Epoch 08] Train Loss: 0.4980 | Val Loss: 0.5345 | mIoU: 0.5181 | Acc: 0.8142\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5181\n",
      "[Fold 5 | Epoch 09] Train Loss: 0.4854 | Val Loss: 0.5320 | mIoU: 0.5278 | Acc: 0.8169\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5278\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.4641 | Val Loss: 0.5240 | mIoU: 0.5402 | Acc: 0.8212\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5402\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.4443 | Val Loss: 0.5140 | mIoU: 0.5369 | Acc: 0.8177\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.4252 | Val Loss: 0.5109 | mIoU: 0.5490 | Acc: 0.8231\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5490\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.3982 | Val Loss: 0.5065 | mIoU: 0.5524 | Acc: 0.8239\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5524\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.3943 | Val Loss: 0.4984 | mIoU: 0.5594 | Acc: 0.8305\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5594\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.3768 | Val Loss: 0.4969 | mIoU: 0.5619 | Acc: 0.8304\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5619\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.3646 | Val Loss: 0.4867 | mIoU: 0.5648 | Acc: 0.8364\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5648\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.3577 | Val Loss: 0.4997 | mIoU: 0.5546 | Acc: 0.8248\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.3458 | Val Loss: 0.5079 | mIoU: 0.5519 | Acc: 0.8178\n",
      "[Fold 5 | Epoch 19] Train Loss: 0.3376 | Val Loss: 0.4866 | mIoU: 0.5610 | Acc: 0.8311\n",
      "[Fold 5 | Epoch 20] Train Loss: 0.3318 | Val Loss: 0.4812 | mIoU: 0.5676 | Acc: 0.8353\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5676\n",
      "[Fold 5 | Epoch 21] Train Loss: 0.3242 | Val Loss: 0.4867 | mIoU: 0.5636 | Acc: 0.8322\n",
      "[Fold 5 | Epoch 22] Train Loss: 0.3222 | Val Loss: 0.4852 | mIoU: 0.5619 | Acc: 0.8304\n",
      "[Fold 5 | Epoch 23] Train Loss: 0.3176 | Val Loss: 0.4772 | mIoU: 0.5735 | Acc: 0.8354\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5735\n",
      "[Fold 5 | Epoch 24] Train Loss: 0.3144 | Val Loss: 0.4781 | mIoU: 0.5735 | Acc: 0.8368\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5735\n",
      "[Fold 5 | Epoch 25] Train Loss: 0.3066 | Val Loss: 0.4804 | mIoU: 0.5648 | Acc: 0.8315\n",
      "[Fold 5 | Epoch 26] Train Loss: 0.3124 | Val Loss: 0.4793 | mIoU: 0.5693 | Acc: 0.8351\n",
      "[Fold 5 | Epoch 27] Train Loss: 0.3043 | Val Loss: 0.4789 | mIoU: 0.5681 | Acc: 0.8343\n",
      "[Fold 5 | Epoch 28] Train Loss: 0.3087 | Val Loss: 0.4819 | mIoU: 0.5654 | Acc: 0.8347\n",
      "[Fold 5 | Epoch 29] Train Loss: 0.3065 | Val Loss: 0.4814 | mIoU: 0.5638 | Acc: 0.8329\n",
      "[Fold 5 | Epoch 30] Train Loss: 0.2984 | Val Loss: 0.4771 | mIoU: 0.5728 | Acc: 0.8360\n",
      "[Fold 5 | Epoch 31] Train Loss: 0.3025 | Val Loss: 0.4804 | mIoU: 0.5663 | Acc: 0.8330\n",
      "[Fold 5 | Epoch 32] Train Loss: 0.2971 | Val Loss: 0.4879 | mIoU: 0.5649 | Acc: 0.8308\n",
      "[Fold 5 | Epoch 33] Train Loss: 0.2939 | Val Loss: 0.4859 | mIoU: 0.5676 | Acc: 0.8331\n",
      "[Fold 5 | Epoch 34] Train Loss: 0.2959 | Val Loss: 0.4810 | mIoU: 0.5642 | Acc: 0.8328\n",
      "[Fold 5 | Epoch 35] Train Loss: 0.2922 | Val Loss: 0.4767 | mIoU: 0.5677 | Acc: 0.8337\n",
      "[Fold 5 | Epoch 36] Train Loss: 0.2910 | Val Loss: 0.4836 | mIoU: 0.5653 | Acc: 0.8323\n",
      "[Fold 5 | Epoch 37] Train Loss: 0.2892 | Val Loss: 0.4839 | mIoU: 0.5594 | Acc: 0.8301\n",
      "[Fold 5 | Epoch 38] Train Loss: 0.2921 | Val Loss: 0.4782 | mIoU: 0.5615 | Acc: 0.8325\n",
      "[Fold 5 | Epoch 39] Train Loss: 0.2918 | Val Loss: 0.4753 | mIoU: 0.5688 | Acc: 0.8348\n",
      "[Fold 5 | Epoch 40] Train Loss: 0.2953 | Val Loss: 0.4769 | mIoU: 0.5688 | Acc: 0.8350\n",
      "[Fold 5 | Epoch 41] Train Loss: 0.2876 | Val Loss: 0.4778 | mIoU: 0.5645 | Acc: 0.8327\n",
      "[Fold 5 | Epoch 42] Train Loss: 0.2918 | Val Loss: 0.4723 | mIoU: 0.5698 | Acc: 0.8359\n",
      "[Fold 5 | Epoch 43] Train Loss: 0.2861 | Val Loss: 0.4781 | mIoU: 0.5695 | Acc: 0.8349\n",
      "[Fold 5 | Epoch 44] Train Loss: 0.2874 | Val Loss: 0.4828 | mIoU: 0.5632 | Acc: 0.8307\n",
      "[Fold 5 | Epoch 45] Train Loss: 0.2874 | Val Loss: 0.4774 | mIoU: 0.5686 | Acc: 0.8335\n",
      "[Fold 5 | Epoch 46] Train Loss: 0.2886 | Val Loss: 0.4741 | mIoU: 0.5722 | Acc: 0.8361\n",
      "[Fold 5 | Epoch 47] Train Loss: 0.2882 | Val Loss: 0.4712 | mIoU: 0.5727 | Acc: 0.8369\n",
      "[Fold 5 | Epoch 48] Train Loss: 0.2875 | Val Loss: 0.4737 | mIoU: 0.5690 | Acc: 0.8352\n",
      "[Fold 5 | Epoch 49] Train Loss: 0.2828 | Val Loss: 0.4703 | mIoU: 0.5735 | Acc: 0.8380\n",
      "[Fold 5 | Epoch 50] Train Loss: 0.2895 | Val Loss: 0.4752 | mIoU: 0.5688 | Acc: 0.8348\n",
      "\n",
      "Cross-validation completata. Tutti i modelli e risultati salvati.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14\")\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import SegmentationDataset\n",
    "from utils.model import DeepLabV3PlusWithMobileNet\n",
    "from utils.losses import get_weighted_crossentropy_loss, combined_loss\n",
    "from utils.metrics import compute_miou, compute_pixel_accuracy\n",
    "\n",
    "# CONFIG\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 9\n",
    "EPOCHS = 20\n",
    "ALPHA = 0.7\n",
    "LR = 1e-4\n",
    "PATIENCE = 10\n",
    "MAX_MEMORY_MB = 5120  # 5 GB\n",
    "\n",
    "# Dataset\n",
    "train_dataset = SegmentationDataset(\n",
    "    root_dir='/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset',\n",
    "    id_list_file='/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_fold1.txt',\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "val_dataset = SegmentationDataset(\n",
    "    root_dir='/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset',\n",
    "    id_list_file='/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/val/val_fold1.txt',\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Device, modello, ottimizzatore\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DeepLabV3PlusWithMobileNet(num_classes=NUM_CLASSES).to(device)\n",
    "ce_loss_fn = get_weighted_crossentropy_loss(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Early stopping + salvataggio su mIoU\n",
    "best_miou = -1.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # VALIDAZIONE\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_miou = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "            val_loss += loss.item()\n",
    "            total_miou += compute_miou(outputs, labels, num_classes=NUM_CLASSES, ignore_index=0)\n",
    "            total_acc += compute_pixel_accuracy(outputs, labels, ignore_index=0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    avg_miou = total_miou / len(val_loader)\n",
    "    avg_acc = total_acc / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"mIoU: {avg_miou:.4f} | Acc: {avg_acc:.4f}\")\n",
    "\n",
    "    # SALVATAGGIO se migliora la mIoU\n",
    "    if avg_miou > best_miou:\n",
    "        best_miou = avg_miou\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"MODELLO SALVATO: nuova mIoU migliore = {best_miou:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"EARLY STOPPING ATTIVATO: nessun miglioramento per {PATIENCE} epoche consecutive.\")\n",
    "            break\n",
    "\n",
    "    # MEMORY CHECK (solo se su CUDA)\n",
    "    if device.type == 'cuda':\n",
    "        mem_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)  # in MB\n",
    "        if mem_allocated > MAX_MEMORY_MB:\n",
    "            print(f\"MEMORIA GPU SUPERATA: {mem_allocated:.2f} MB > {MAX_MEMORY_MB} MB\")\n",
    "            break\n",
    "\n",
    "    # Cleanup memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ],
   "metadata": {
    "id": "lMAMGEDUyTDZ",
    "ExecuteTime": {
     "start_time": "2025-07-03T12:31:43.922728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss: 1.2090 | Val Loss: 1.0285 | mIoU: 0.3656 | Acc: 0.7268\n",
      "MODELLO SALVATO: nuova mIoU migliore = 0.3656\n",
      "Epoch 02/20 | Train Loss: 0.9399 | Val Loss: 0.8520 | mIoU: 0.3857 | Acc: 0.7423\n",
      "MODELLO SALVATO: nuova mIoU migliore = 0.3857\n",
      "Epoch 03/20 | Train Loss: 0.8016 | Val Loss: 0.8063 | mIoU: 0.4191 | Acc: 0.7339\n",
      "MODELLO SALVATO: nuova mIoU migliore = 0.4191\n",
      "Epoch 04/20 | Train Loss: 0.7095 | Val Loss: 0.7153 | mIoU: 0.4439 | Acc: 0.7386\n",
      "MODELLO SALVATO: nuova mIoU migliore = 0.4439\n",
      "Epoch 05/20 | Train Loss: 0.6383 | Val Loss: 0.6961 | mIoU: 0.4661 | Acc: 0.7519\n",
      "MODELLO SALVATO: nuova mIoU migliore = 0.4661\n",
      "Epoch 06/20 | Train Loss: 0.5797 | Val Loss: 0.6402 | mIoU: 0.4705 | Acc: 0.7571\n",
      "MODELLO SALVATO: nuova mIoU migliore = 0.4705\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:50:56.911123Z",
     "start_time": "2025-07-02T16:50:27.177884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Percorsi\n",
    "DATASET_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset\"\n",
    "TXT_PATH = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_fold5.txt\"\n",
    "OUTPUT_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/augmented_train_fold5\"\n",
    "NEW_TXT_PATH = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_augmented_fold5.txt\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Augmentazioni\n",
    "image_only_transform = A.Compose([\n",
    "    A.ColorJitter(p=0.5)\n",
    "])\n",
    "joint_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.8),\n",
    "    A.RandomRotate90(p=0.3)\n",
    "])\n",
    "\n",
    "# Lista di nomi da salvare nel nuovo txt\n",
    "all_sample_ids = []\n",
    "\n",
    "# Campioni da augmentare\n",
    "with open(TXT_PATH, 'r') as f:\n",
    "    sample_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "for sample_id in tqdm(sample_ids, desc=\"Saving original + augment\"):\n",
    "    input_dir = os.path.join(DATASET_DIR, sample_id)\n",
    "    img_path = os.path.join(input_dir, \"rgb.jpg\")\n",
    "    lbl_path = os.path.join(input_dir, \"labels.png\")\n",
    "\n",
    "    if not os.path.exists(img_path) or not os.path.exists(lbl_path):\n",
    "        print(f\"‚ö†Ô∏è File mancante: {sample_id}\")\n",
    "        continue\n",
    "\n",
    "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    label = np.array(Image.open(lbl_path))  # gi√† in ID, pu√≤ restare come array intero\n",
    "\n",
    "    # --- Originale ---\n",
    "    orig_out_dir = os.path.join(OUTPUT_DIR, sample_id)\n",
    "    os.makedirs(orig_out_dir, exist_ok=True)\n",
    "    Image.fromarray(image).save(os.path.join(orig_out_dir, \"rgb.jpg\"))\n",
    "    Image.fromarray(label.astype(\"uint8\")).save(os.path.join(orig_out_dir, \"labels.png\"))\n",
    "    all_sample_ids.append(sample_id)\n",
    "\n",
    "    # --- Augmentato ---\n",
    "    joint = joint_transform(image=image, mask=label)\n",
    "    aug_image = joint[\"image\"]\n",
    "    aug_label = joint[\"mask\"]\n",
    "    aug_image = image_only_transform(image=aug_image)[\"image\"]\n",
    "\n",
    "    aug_sample_id = f\"{sample_id}_aug\"\n",
    "    aug_out_dir = os.path.join(OUTPUT_DIR, aug_sample_id)\n",
    "    os.makedirs(aug_out_dir, exist_ok=True)\n",
    "    Image.fromarray(aug_image).save(os.path.join(aug_out_dir, \"rgb.jpg\"))\n",
    "    Image.fromarray(aug_label.astype(\"uint8\")).save(os.path.join(aug_out_dir, \"labels.png\"))\n",
    "    all_sample_ids.append(aug_sample_id)\n",
    "\n",
    "# Scrivi il nuovo file .txt\n",
    "with open(NEW_TXT_PATH, 'w') as f:\n",
    "    for sample_id in all_sample_ids:\n",
    "        f.write(sample_id + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Completato! Salvato nuovo file: {NEW_TXT_PATH}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving original + augment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670/670 [00:29<00:00, 22.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completato! Salvato nuovo file: /Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_augmented_fold5.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T17:41:49.725680Z",
     "start_time": "2025-07-02T17:41:26.192994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Percorsi\n",
    "DATASET_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset\"\n",
    "TXT_PATH = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_fold5.txt\"\n",
    "OUTPUT_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/augmented456_train_fold5\"\n",
    "NEW_TXT_PATH = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_augmented456_fold5.txt\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Augmentazioni\n",
    "image_only_transform = A.Compose([\n",
    "    A.ColorJitter(p=0.5)\n",
    "])\n",
    "joint_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.08, scale_limit=0.2, rotate_limit=15, border_mode=0, p=0.7),\n",
    "])\n",
    "\n",
    "image_only_transform = A.Compose([\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1, p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.5)\n",
    "])\n",
    "\n",
    "# Classi da considerare per augment mirata\n",
    "TARGET_CLASSES = {4, 5, 6}\n",
    "\n",
    "# Lista di nomi da salvare nel nuovo txt\n",
    "all_sample_ids = []\n",
    "\n",
    "# Campioni da augmentare\n",
    "with open(TXT_PATH, 'r') as f:\n",
    "    sample_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "for sample_id in tqdm(sample_ids, desc=\"Saving original + selective augment\"):\n",
    "    input_dir = os.path.join(DATASET_DIR, sample_id)\n",
    "    img_path = os.path.join(input_dir, \"rgb.jpg\")\n",
    "    lbl_path = os.path.join(input_dir, \"labels.png\")\n",
    "\n",
    "    if not os.path.exists(img_path) or not os.path.exists(lbl_path):\n",
    "        print(f\"‚ö†Ô∏è File mancante: {sample_id}\")\n",
    "        continue\n",
    "\n",
    "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    label = np.array(Image.open(lbl_path))  # gi√† in ID\n",
    "\n",
    "    # --- Originale ---\n",
    "    orig_out_dir = os.path.join(OUTPUT_DIR, sample_id)\n",
    "    os.makedirs(orig_out_dir, exist_ok=True)\n",
    "    Image.fromarray(image).save(os.path.join(orig_out_dir, \"rgb.jpg\"))\n",
    "    Image.fromarray(label.astype(\"uint8\")).save(os.path.join(orig_out_dir, \"labels.png\"))\n",
    "    all_sample_ids.append(sample_id)\n",
    "\n",
    "    # Verifica se contiene almeno una delle classi 4,5,6\n",
    "    if not TARGET_CLASSES.intersection(np.unique(label)):\n",
    "        continue  # Skip augment se non contiene le classi target\n",
    "\n",
    "    # --- Augmentato ---\n",
    "    joint = joint_transform(image=image, mask=label)\n",
    "    aug_image = joint[\"image\"]\n",
    "    aug_label = joint[\"mask\"]\n",
    "    aug_image = image_only_transform(image=aug_image)[\"image\"]\n",
    "\n",
    "    aug_sample_id = f\"{sample_id}_aug\"\n",
    "    aug_out_dir = os.path.join(OUTPUT_DIR, aug_sample_id)\n",
    "    os.makedirs(aug_out_dir, exist_ok=True)\n",
    "    Image.fromarray(aug_image).save(os.path.join(aug_out_dir, \"rgb.jpg\"))\n",
    "    Image.fromarray(aug_label.astype(\"uint8\")).save(os.path.join(aug_out_dir, \"labels.png\"))\n",
    "    all_sample_ids.append(aug_sample_id)\n",
    "\n",
    "# Scrivi il nuovo file .txt\n",
    "with open(NEW_TXT_PATH, 'w') as f:\n",
    "    for sample_id in all_sample_ids:\n",
    "        f.write(sample_id + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Completato! Salvato nuovo file: {NEW_TXT_PATH}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasyc\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "Saving original + selective augment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670/670 [00:23<00:00, 28.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completato! Salvato nuovo file: /Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_augmented456_fold5.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T17:44:08.110880Z",
     "start_time": "2025-07-02T17:43:49.036752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#ANALISI DATASET, RESTITUISCE UN DATA\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Importa la funzione di analisi\n",
    "from utils.analysis_utils import analyze_class_distribution\n",
    "\n",
    "# Specifica il percorso del dataset\n",
    "DATASET_DIR = '/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/augmented456_train_fold5'\n",
    "\n",
    "# Esegui l'analisi\n",
    "df = analyze_class_distribution(DATASET_DIR)\n",
    "\n",
    "# Mostra il DataFrame in forma tabellare\n",
    "df.style.set_caption(\"Distribuzione delle Classi\").format(precision=2)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1014/1014 [00:19<00:00, 53.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a4b44bb730>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9807b\">\n",
       "  <caption>Distribuzione delle Classi</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9807b_level0_col0\" class=\"col_heading level0 col0\" >Class</th>\n",
       "      <th id=\"T_9807b_level0_col1\" class=\"col_heading level0 col1\" >Pixel %</th>\n",
       "      <th id=\"T_9807b_level0_col2\" class=\"col_heading level0 col2\" >Presence % (images)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9807b_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_9807b_row0_col1\" class=\"data row0 col1\" >5.98</td>\n",
       "      <td id=\"T_9807b_row0_col2\" class=\"data row0 col2\" >99.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9807b_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_9807b_row1_col1\" class=\"data row1 col1\" >15.45</td>\n",
       "      <td id=\"T_9807b_row1_col2\" class=\"data row1 col2\" >60.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9807b_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_9807b_row2_col1\" class=\"data row2 col1\" >11.78</td>\n",
       "      <td id=\"T_9807b_row2_col2\" class=\"data row2 col2\" >64.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_9807b_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_9807b_row3_col1\" class=\"data row3 col1\" >14.34</td>\n",
       "      <td id=\"T_9807b_row3_col2\" class=\"data row3 col2\" >55.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_9807b_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_9807b_row4_col1\" class=\"data row4 col1\" >0.25</td>\n",
       "      <td id=\"T_9807b_row4_col2\" class=\"data row4 col2\" >6.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_9807b_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_9807b_row5_col1\" class=\"data row5 col1\" >1.11</td>\n",
       "      <td id=\"T_9807b_row5_col2\" class=\"data row5 col2\" >23.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_9807b_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_9807b_row6_col1\" class=\"data row6 col1\" >7.88</td>\n",
       "      <td id=\"T_9807b_row6_col2\" class=\"data row6 col2\" >50.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_9807b_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_9807b_row7_col1\" class=\"data row7 col1\" >33.66</td>\n",
       "      <td id=\"T_9807b_row7_col2\" class=\"data row7 col2\" >99.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9807b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_9807b_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_9807b_row8_col1\" class=\"data row8 col1\" >9.55</td>\n",
       "      <td id=\"T_9807b_row8_col2\" class=\"data row8 col2\" >90.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T19:11:39.262835Z",
     "start_time": "2025-07-02T17:50:57.413192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Imposta seed per riproducibilit√†\n",
    "SEED = 14\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import SegmentationDataset\n",
    "from utils.model import DeepLabV3PlusWithMobileNet\n",
    "from utils.losses import get_weighted_crossentropy_loss, combined_loss\n",
    "from utils.metrics import compute_miou, compute_pixel_accuracy\n",
    "\n",
    "# CONFIG\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 9\n",
    "EPOCHS = 50\n",
    "ALPHA = 0.7\n",
    "LR = 1e-4\n",
    "MIN_DELTA = 1e-4\n",
    "PATIENCE = 10\n",
    "MAX_MEMORY_MB = 5120  # 5 GB\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold in [5]:\n",
    "    print(f\"\\n Inizio Fold {fold}\\n{'-'*40}\")\n",
    "\n",
    "    train_dataset = SegmentationDataset(\n",
    "        root_dir=\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/augmented456_train_fold5\",\n",
    "        id_list_file=f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_augmented456_fold{fold}.txt\",\n",
    "        image_size=IMAGE_SIZE,\n",
    "    )\n",
    "    val_dataset = SegmentationDataset(\n",
    "        root_dir=\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset\",\n",
    "        id_list_file=f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/val/val_fold{fold}.txt\",\n",
    "        image_size=IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = DeepLabV3PlusWithMobileNet(num_classes=NUM_CLASSES).to(device)\n",
    "    ce_loss_fn = get_weighted_crossentropy_loss(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    best_miou = -1.0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    fold_results = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_miou = 0.0\n",
    "        total_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "                val_loss += loss.item()\n",
    "                total_miou += compute_miou(outputs, labels, num_classes=NUM_CLASSES, ignore_index=0)\n",
    "                total_acc += compute_pixel_accuracy(outputs, labels, ignore_index=0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        avg_miou = total_miou / len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        avg_acc = total_acc / len(val_loader)\n",
    "\n",
    "        fold_results.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"mIoU\": avg_miou,\n",
    "            \"accuracy\": avg_acc\n",
    "        })\n",
    "\n",
    "        print(f\"[Fold {fold} | Epoch {epoch:02d}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "              f\"mIoU: {avg_miou:.4f} | Acc: {avg_acc:.4f}\")\n",
    "\n",
    "        if avg_miou > best_miou:\n",
    "            best_miou = avg_miou\n",
    "            epochs_no_improve = 0\n",
    "            model_path = f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/best_model_fold{fold}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Salvataggio modello Fold {fold} | Nuova mIoU: {best_miou:.4f}\")\n",
    "\n",
    "        if epoch == 1 or val_loss < best_val_loss - MIN_DELTA:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"EARLY STOPPING FOLD {fold}: no val_loss improvement for {PATIENCE} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            mem_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            if mem_allocated > MAX_MEMORY_MB:\n",
    "                print(f\"GPU MEMORY LIMIT EXCEEDED: {mem_allocated:.2f} MB\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    RESULTS.append({\n",
    "        \"fold\": fold,\n",
    "        \"best_miou\": best_miou,\n",
    "        \"epoch_results\": fold_results\n",
    "    })\n",
    "\n",
    "# ‚úÖ Salva i risultati totali in un file\n",
    "import json\n",
    "with open(\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_results.json\", \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "\n",
    "print(\"\\nCross-validation completata. Tutti i modelli e risultati salvati.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inizio Fold 5\n",
      "----------------------------------------\n",
      "[Fold 5 | Epoch 01] Train Loss: 1.1204 | Val Loss: 0.8438 | mIoU: 0.4238 | Acc: 0.7474\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.4238\n",
      "[Fold 5 | Epoch 02] Train Loss: 0.8475 | Val Loss: 0.6918 | mIoU: 0.5183 | Acc: 0.8159\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5183\n",
      "[Fold 5 | Epoch 03] Train Loss: 0.7171 | Val Loss: 0.6011 | mIoU: 0.5153 | Acc: 0.8103\n",
      "[Fold 5 | Epoch 04] Train Loss: 0.6309 | Val Loss: 0.5660 | mIoU: 0.5087 | Acc: 0.8076\n",
      "[Fold 5 | Epoch 05] Train Loss: 0.5712 | Val Loss: 0.5420 | mIoU: 0.5189 | Acc: 0.8042\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5189\n",
      "[Fold 5 | Epoch 06] Train Loss: 0.5196 | Val Loss: 0.5305 | mIoU: 0.5369 | Acc: 0.8114\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5369\n",
      "[Fold 5 | Epoch 07] Train Loss: 0.4857 | Val Loss: 0.5280 | mIoU: 0.5283 | Acc: 0.8021\n",
      "[Fold 5 | Epoch 08] Train Loss: 0.4326 | Val Loss: 0.4848 | mIoU: 0.5565 | Acc: 0.8292\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5565\n",
      "[Fold 5 | Epoch 09] Train Loss: 0.4087 | Val Loss: 0.4663 | mIoU: 0.5649 | Acc: 0.8352\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5649\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.3869 | Val Loss: 0.4811 | mIoU: 0.5620 | Acc: 0.8289\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.3756 | Val Loss: 0.4655 | mIoU: 0.5646 | Acc: 0.8323\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.3562 | Val Loss: 0.4760 | mIoU: 0.5593 | Acc: 0.8267\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.3434 | Val Loss: 0.4694 | mIoU: 0.5624 | Acc: 0.8309\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:55:14.169545Z",
     "start_time": "2025-07-03T17:54:27.546979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from utils.color_utils import save_indexed_label_with_palette\n",
    "\n",
    "# --- Bounding box con margine ---\n",
    "def get_bbox_with_margin(mask, target_class, margin=0.25):\n",
    "    ys, xs = np.where(mask == target_class)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None\n",
    "    x_min, x_max = xs.min(), xs.max()\n",
    "    y_min, y_max = ys.min(), ys.max()\n",
    "\n",
    "    h, w = mask.shape\n",
    "    bw = x_max - x_min\n",
    "    bh = y_max - y_min\n",
    "    if bw < 30 or bh < 30:\n",
    "        return None\n",
    "\n",
    "    pad_x = int(bw * margin)\n",
    "    pad_y = int(bh * margin)\n",
    "\n",
    "    x_min = max(0, x_min - pad_x)\n",
    "    x_max = min(w, x_max + pad_x)\n",
    "    y_min = max(0, y_min - pad_y)\n",
    "    y_max = min(h, y_max + pad_y)\n",
    "\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "# --- Augmentation per immagine originale ---\n",
    "def augment_original_image(image):\n",
    "    aug = A.OneOf([\n",
    "        A.Sharpen(alpha=(0.1, 0.3), lightness=(0.9, 1.0), kernel_size=7, p=1),\n",
    "        A.ColorJitter(brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=[0.8, 1.2], hue=0.05, p=1),\n",
    "        A.CLAHE(clip_limit=7.0, tile_grid_size=(12,12), p=1),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.2, 0.2], contrast_limit=[-0.2, 0.2], p=1),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=1),\n",
    "    ], p=1.0)\n",
    "    return aug(image=image)[\"image\"]\n",
    "\n",
    "# --- CLAHE su immagine con probabilit√† ---\n",
    "def apply_clahe_with_probability(image, prob=0.7):\n",
    "    if np.random.rand() < prob:\n",
    "        return A.CLAHE(clip_limit=7.0, tile_grid_size=(12, 12), p=1)(image=image)[\"image\"]\n",
    "    return image\n",
    "\n",
    "# --- Funzione di zoom + flip ---\n",
    "def zoom_and_flip(image, label, cls_id, sample_id, output_dir):\n",
    "    bbox = get_bbox_with_margin(label, cls_id)\n",
    "    if bbox is None:\n",
    "        return []\n",
    "\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    cropped_img = image[y_min:y_max, x_min:x_max]\n",
    "    cropped_lbl = label[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    resize = A.Resize(320, 320, interpolation=Image.BILINEAR)\n",
    "    resized_img = resize(image=cropped_img)[\"image\"]\n",
    "    resized_lbl = A.Resize(320, 320, interpolation=Image.NEAREST)(image=cropped_lbl)[\"image\"]\n",
    "\n",
    "    saved_ids = []\n",
    "\n",
    "    # Zoomato normale\n",
    "    zoom_id = f\"{sample_id}_zoom{cls_id}\"\n",
    "    zoom_dir = os.path.join(output_dir, zoom_id)\n",
    "    os.makedirs(zoom_dir, exist_ok=True)\n",
    "    Image.fromarray(resized_img).save(os.path.join(zoom_dir, \"rgb.jpg\"))\n",
    "    save_indexed_label_with_palette(resized_lbl.astype(\"uint8\"), os.path.join(zoom_dir, \"labels.png\"))\n",
    "    saved_ids.append(zoom_id)\n",
    "\n",
    "    # CLAHE sullo zoomato (probabilmente)\n",
    "    clahe_img = apply_clahe_with_probability(resized_img)\n",
    "    clahe_id = f\"{zoom_id}_clahe\"\n",
    "    clahe_dir = os.path.join(output_dir, clahe_id)\n",
    "    os.makedirs(clahe_dir, exist_ok=True)\n",
    "    Image.fromarray(clahe_img).save(os.path.join(clahe_dir, \"rgb.jpg\"))\n",
    "    save_indexed_label_with_palette(resized_lbl.astype(\"uint8\"), os.path.join(clahe_dir, \"labels.png\"))\n",
    "    saved_ids.append(clahe_id)\n",
    "\n",
    "    # Flip\n",
    "    flip = A.HorizontalFlip(p=1.0)\n",
    "    flipped_img = flip(image=resized_img)[\"image\"]\n",
    "    flipped_lbl = flip(image=resized_lbl)[\"image\"]\n",
    "\n",
    "    flip_id = f\"{zoom_id}_flip\"\n",
    "    flip_dir = os.path.join(output_dir, flip_id)\n",
    "    os.makedirs(flip_dir, exist_ok=True)\n",
    "    Image.fromarray(flipped_img).save(os.path.join(flip_dir, \"rgb.jpg\"))\n",
    "    save_indexed_label_with_palette(flipped_lbl.astype(\"uint8\"), os.path.join(flip_dir, \"labels.png\"))\n",
    "    saved_ids.append(flip_id)\n",
    "\n",
    "    # CLAHE sul flipped\n",
    "    clahe_flipped_img = apply_clahe_with_probability(flipped_img)\n",
    "    clahe_flip_id = f\"{flip_id}_clahe\"\n",
    "    clahe_flip_dir = os.path.join(output_dir, clahe_flip_id)\n",
    "    os.makedirs(clahe_flip_dir, exist_ok=True)\n",
    "    Image.fromarray(clahe_flipped_img).save(os.path.join(clahe_flip_dir, \"rgb.jpg\"))\n",
    "    save_indexed_label_with_palette(flipped_lbl.astype(\"uint8\"), os.path.join(clahe_flip_dir, \"labels.png\"))\n",
    "    saved_ids.append(clahe_flip_id)\n",
    "\n",
    "    return saved_ids\n",
    "\n",
    "# --- Percorsi ---\n",
    "DATASET_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset\"\n",
    "TXT_PATH = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/train_fold5.txt\"\n",
    "OUTPUT_DIR = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/augmented_zoom_flip_clahe_fold5\"\n",
    "NEW_TXT_PATH = \"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/augmented_zoom_flip_clahe_fold5.txt\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "ZOOM_CLASSES = [4, 5]\n",
    "all_sample_ids = []\n",
    "\n",
    "with open(TXT_PATH, 'r') as f:\n",
    "    sample_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "for sample_id in tqdm(sample_ids, desc=\"Augment + Zoom + Flip + CLAHE\"):\n",
    "    input_dir = os.path.join(DATASET_DIR, sample_id)\n",
    "    img_path = os.path.join(input_dir, \"rgb.jpg\")\n",
    "    lbl_path = os.path.join(input_dir, \"labels.png\")\n",
    "\n",
    "    if not os.path.exists(img_path) or not os.path.exists(lbl_path):\n",
    "        print(f\"‚ö† File mancante: {sample_id}\")\n",
    "        continue\n",
    "\n",
    "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    label = np.array(Image.open(lbl_path))\n",
    "\n",
    "    # Originale\n",
    "    orig_dir = os.path.join(OUTPUT_DIR, sample_id)\n",
    "    os.makedirs(orig_dir, exist_ok=True)\n",
    "    Image.fromarray(image).save(os.path.join(orig_dir, \"rgb.jpg\"))\n",
    "    save_indexed_label_with_palette(label.astype(\"uint8\"), os.path.join(orig_dir, \"labels.png\"))\n",
    "    all_sample_ids.append(sample_id)\n",
    "\n",
    "    # Versione augmentata\n",
    "    aug_image = augment_original_image(image)\n",
    "    aug_id = f\"{sample_id}_aug\"\n",
    "    aug_dir = os.path.join(OUTPUT_DIR, aug_id)\n",
    "    os.makedirs(aug_dir, exist_ok=True)\n",
    "    Image.fromarray(aug_image).save(os.path.join(aug_dir, \"rgb.jpg\"))\n",
    "    save_indexed_label_with_palette(label.astype(\"uint8\"), os.path.join(aug_dir, \"labels.png\"))\n",
    "    all_sample_ids.append(aug_id)\n",
    "\n",
    "    # Zoom + flip + CLAHE su classi 4 e 5\n",
    "    for cls_id in ZOOM_CLASSES:\n",
    "        new_ids = zoom_and_flip(image, label, cls_id, sample_id, OUTPUT_DIR)\n",
    "        all_sample_ids.extend(new_ids)\n",
    "\n",
    "# --- Salva nuovo TXT ---\n",
    "with open(NEW_TXT_PATH, 'w') as f:\n",
    "    for sid in all_sample_ids:\n",
    "        f.write(sid + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Completato. Salvati {len(all_sample_ids)} campioni in: {NEW_TXT_PATH}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augment + Zoom + Flip + CLAHE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670/670 [00:46<00:00, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completato. Salvati 1792 campioni in: /Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/augmented_zoom_flip_clahe_fold5.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T21:05:26.865278Z",
     "start_time": "2025-07-03T17:56:34.487687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Imposta seed per riproducibilit√†\n",
    "SEED = 14\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import SegmentationDataset\n",
    "from utils.model import DeepLabV3PlusWithMobileNet\n",
    "from utils.losses import get_weighted_crossentropy_loss, combined_loss\n",
    "from utils.metrics import compute_miou, compute_pixel_accuracy\n",
    "\n",
    "# CONFIG\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 9\n",
    "EPOCHS = 50\n",
    "ALPHA = 0.7\n",
    "LR = 1e-4\n",
    "MIN_DELTA = 1e-4\n",
    "PATIENCE = 10\n",
    "MAX_MEMORY_MB = 5120  # 5 GB\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold in [5]:\n",
    "    print(f\"\\n Inizio Fold {fold}\\n{'-'*40}\")\n",
    "\n",
    "    train_dataset = SegmentationDataset(\n",
    "        root_dir=\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/augmented_zoom_flip_clahe_fold5\",\n",
    "        id_list_file=f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/train/augmented_zoom_flip_clahe_fold{fold}.txt\",\n",
    "        image_size=IMAGE_SIZE,\n",
    "    )\n",
    "    val_dataset = SegmentationDataset(\n",
    "        root_dir=\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/dataset/dataset\",\n",
    "        id_list_file=f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_splits/val/val_fold{fold}.txt\",\n",
    "        image_size=IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = DeepLabV3PlusWithMobileNet(num_classes=NUM_CLASSES).to(device)\n",
    "    ce_loss_fn = get_weighted_crossentropy_loss(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    best_miou = -1.0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    fold_results = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_miou = 0.0\n",
    "        total_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, ce_loss_fn, alpha=ALPHA)\n",
    "                val_loss += loss.item()\n",
    "                total_miou += compute_miou(outputs, labels, num_classes=NUM_CLASSES, ignore_index=0)\n",
    "                total_acc += compute_pixel_accuracy(outputs, labels, ignore_index=0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        avg_miou = total_miou / len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        avg_acc = total_acc / len(val_loader)\n",
    "\n",
    "        fold_results.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"mIoU\": avg_miou,\n",
    "            \"accuracy\": avg_acc\n",
    "        })\n",
    "\n",
    "        print(f\"[Fold {fold} | Epoch {epoch:02d}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "              f\"mIoU: {avg_miou:.4f} | Acc: {avg_acc:.4f}\")\n",
    "\n",
    "        if avg_miou > best_miou:\n",
    "            best_miou = avg_miou\n",
    "            epochs_no_improve = 0\n",
    "            model_path = f\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/best_model_fold{fold}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Salvataggio modello Fold {fold} | Nuova mIoU: {best_miou:.4f}\")\n",
    "\n",
    "        if epoch == 1 or val_loss < best_val_loss - MIN_DELTA:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"EARLY STOPPING FOLD {fold}: no val_loss improvement for {PATIENCE} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            mem_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            if mem_allocated > MAX_MEMORY_MB:\n",
    "                print(f\"GPU MEMORY LIMIT EXCEEDED: {mem_allocated:.2f} MB\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    RESULTS.append({\n",
    "        \"fold\": fold,\n",
    "        \"best_miou\": best_miou,\n",
    "        \"epoch_results\": fold_results\n",
    "    })\n",
    "\n",
    "# ‚úÖ Salva i risultati totali in un file\n",
    "import json\n",
    "with open(\"/Users/sasyc/Downloads/ML Project Work/2025_ml_gr14/2025_ml_gr14/kfold_results.json\", \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "\n",
    "print(\"\\nCross-validation completata. Tutti i modelli e risultati salvati.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inizio Fold 5\n",
      "----------------------------------------\n",
      "[Fold 5 | Epoch 01] Train Loss: 1.0728 | Val Loss: 0.7424 | mIoU: 0.4827 | Acc: 0.7827\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.4827\n",
      "[Fold 5 | Epoch 02] Train Loss: 0.7765 | Val Loss: 0.5820 | mIoU: 0.5111 | Acc: 0.8095\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5111\n",
      "[Fold 5 | Epoch 03] Train Loss: 0.6575 | Val Loss: 0.5599 | mIoU: 0.5142 | Acc: 0.8054\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5142\n",
      "[Fold 5 | Epoch 04] Train Loss: 0.5773 | Val Loss: 0.5318 | mIoU: 0.4921 | Acc: 0.7830\n",
      "[Fold 5 | Epoch 05] Train Loss: 0.5164 | Val Loss: 0.4980 | mIoU: 0.5081 | Acc: 0.8026\n",
      "[Fold 5 | Epoch 06] Train Loss: 0.4601 | Val Loss: 0.4904 | mIoU: 0.5379 | Acc: 0.8172\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5379\n",
      "[Fold 5 | Epoch 07] Train Loss: 0.4146 | Val Loss: 0.4621 | mIoU: 0.5526 | Acc: 0.8319\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5526\n",
      "[Fold 5 | Epoch 08] Train Loss: 0.3641 | Val Loss: 0.4650 | mIoU: 0.5455 | Acc: 0.8298\n",
      "[Fold 5 | Epoch 09] Train Loss: 0.3365 | Val Loss: 0.4593 | mIoU: 0.5548 | Acc: 0.8351\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5548\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.3102 | Val Loss: 0.4644 | mIoU: 0.5579 | Acc: 0.8355\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5579\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.2980 | Val Loss: 0.4869 | mIoU: 0.5445 | Acc: 0.8278\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.2846 | Val Loss: 0.4783 | mIoU: 0.5427 | Acc: 0.8292\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.2734 | Val Loss: 0.4737 | mIoU: 0.5621 | Acc: 0.8338\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5621\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.2566 | Val Loss: 0.4962 | mIoU: 0.5466 | Acc: 0.8242\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.2460 | Val Loss: 0.4691 | mIoU: 0.5594 | Acc: 0.8401\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.2387 | Val Loss: 0.4714 | mIoU: 0.5607 | Acc: 0.8356\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.2337 | Val Loss: 0.4739 | mIoU: 0.5583 | Acc: 0.8371\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.2302 | Val Loss: 0.4725 | mIoU: 0.5646 | Acc: 0.8397\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5646\n",
      "[Fold 5 | Epoch 19] Train Loss: 0.2221 | Val Loss: 0.4876 | mIoU: 0.5522 | Acc: 0.8326\n",
      "[Fold 5 | Epoch 20] Train Loss: 0.2157 | Val Loss: 0.4740 | mIoU: 0.5559 | Acc: 0.8398\n",
      "[Fold 5 | Epoch 21] Train Loss: 0.2110 | Val Loss: 0.4725 | mIoU: 0.5644 | Acc: 0.8433\n",
      "[Fold 5 | Epoch 22] Train Loss: 0.2079 | Val Loss: 0.4739 | mIoU: 0.5592 | Acc: 0.8440\n",
      "[Fold 5 | Epoch 23] Train Loss: 0.2091 | Val Loss: 0.4820 | mIoU: 0.5606 | Acc: 0.8400\n",
      "[Fold 5 | Epoch 24] Train Loss: 0.2018 | Val Loss: 0.4911 | mIoU: 0.5497 | Acc: 0.8374\n",
      "[Fold 5 | Epoch 25] Train Loss: 0.2010 | Val Loss: 0.4801 | mIoU: 0.5626 | Acc: 0.8418\n",
      "[Fold 5 | Epoch 26] Train Loss: 0.2001 | Val Loss: 0.4765 | mIoU: 0.5610 | Acc: 0.8420\n",
      "[Fold 5 | Epoch 27] Train Loss: 0.1970 | Val Loss: 0.4820 | mIoU: 0.5671 | Acc: 0.8441\n",
      "Salvataggio modello Fold 5 | Nuova mIoU: 0.5671\n",
      "[Fold 5 | Epoch 28] Train Loss: 0.1937 | Val Loss: 0.4795 | mIoU: 0.5623 | Acc: 0.8429\n",
      "[Fold 5 | Epoch 29] Train Loss: 0.1935 | Val Loss: 0.4899 | mIoU: 0.5571 | Acc: 0.8402\n",
      "[Fold 5 | Epoch 30] Train Loss: 0.1928 | Val Loss: 0.4872 | mIoU: 0.5623 | Acc: 0.8409\n",
      "[Fold 5 | Epoch 31] Train Loss: 0.1949 | Val Loss: 0.4898 | mIoU: 0.5549 | Acc: 0.8401\n",
      "[Fold 5 | Epoch 32] Train Loss: 0.1907 | Val Loss: 0.4868 | mIoU: 0.5606 | Acc: 0.8416\n",
      "[Fold 5 | Epoch 33] Train Loss: 0.1893 | Val Loss: 0.4835 | mIoU: 0.5596 | Acc: 0.8422\n",
      "[Fold 5 | Epoch 34] Train Loss: 0.1878 | Val Loss: 0.4869 | mIoU: 0.5606 | Acc: 0.8416\n",
      "[Fold 5 | Epoch 35] Train Loss: 0.1905 | Val Loss: 0.4913 | mIoU: 0.5601 | Acc: 0.8406\n",
      "[Fold 5 | Epoch 36] Train Loss: 0.1861 | Val Loss: 0.4902 | mIoU: 0.5536 | Acc: 0.8407\n",
      "EARLY STOPPING FOLD 5: no val_loss improvement for 10 epochs.\n",
      "\n",
      "Cross-validation completata. Tutti i modelli e risultati salvati.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ]
}
